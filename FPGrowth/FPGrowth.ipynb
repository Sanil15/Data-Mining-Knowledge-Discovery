{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.mllib.fpm import FPGrowth\n",
    "from operator import itemgetter\n",
    "\n",
    "SparkContext.setSystemProperty('spark.executor.memory','6g')\n",
    "sc = pyspark.SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "rdd = sc.textFile('publications.txt')\n",
    "small_rdd = rdd.sample(False, 1e-3)\n",
    "\n",
    "# RDDs for statistical analysis and further calculations\n",
    "venue_rdd = rdd.filter(\n",
    "            lambda l: re.match('^#c(.*)',l)).map(\n",
    "            lambda l: re.match('^#c(.*)',l).group(1))\n",
    "\n",
    "refrence_rdd = rdd.filter(\n",
    "            lambda l: re.match('^#%(.*)',l)).map(\n",
    "            lambda l: re.match('^#%(.*)',l).group(1)).filter(lambda l: l!='')\n",
    "\n",
    "author_rdd = rdd.filter(\n",
    "            lambda l: re.match('^#@(.*)',l)).map(\n",
    "            lambda l: re.match('^#@(.*)',l).group(1))\n",
    "\n",
    "year_rdd = rdd.filter(\n",
    "            lambda l: re.match('^#t(.*)',l)).map(\n",
    "            lambda l: re.match('^#t(.*)',l).group(1)).filter(lambda l: l!='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# RDD for authors\n",
    "transactions = author_rdd.filter(lambda l: l.strip() != \"\").map(lambda line:list(set(line.strip().split(','))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUESTION 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To parse publications.txt I have used sed and awk and I have got the following results:\n",
    "\n",
    "\n",
    "- Index is at not the first line for every record in Publications.txt while in AP_Train.txt it was the very first line.\n",
    "\n",
    "- Every line in publications.txt has uses nothing as delimeter to seprate the identifying symbols with the string it identifies.\n",
    "\n",
    "- Publications.txt does have empty refrences many records while AP_Train.txt had refrences for some records.\n",
    "\n",
    "- Authors are delimeted by ',' in publications.txt while ';' in AP_Train.txt\n",
    "\n",
    "\n",
    "\n",
    "Difference in number of number of publications, authors, venues, references, and years of publication for two files\n",
    "\n",
    "\n",
    "|              Counts Of              | Publications.txt | AP_train.txt |\n",
    "|:-----------------------------------:|:----------------:|:------------:|\n",
    "| Venue (Unique Count)                |       8708       |    255685    |\n",
    "| Publications/Indexes (Unique Count) |      2146341     |    1976815   |\n",
    "| Authors (Unique Count)              |      1232494     |    1478733   |\n",
    "| Refrences                           |     528264       |    871089    |\n",
    "| Years                               |        80        |      79      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUESTION 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FreqItemset(items=['David Maier'], freq=227)\n",
      "FreqItemset(items=['Michael T. Goodrich'], freq=309)\n",
      "FreqItemset(items=['Ralf Steinmetz'], freq=373)\n",
      "FreqItemset(items=['Wayne Wolf'], freq=243)\n",
      "FreqItemset(items=['Hussein T. Mouftah'], freq=237)\n",
      "FreqItemset(items=['Micha Sharir'], freq=466)\n",
      "FreqItemset(items=['Jin Li'], freq=254)\n",
      "FreqItemset(items=['Ying Wang'], freq=352)\n",
      "FreqItemset(items=['Kiyohiro Shikano'], freq=213)\n",
      "FreqItemset(items=['Gene Tsudik'], freq=220)\n"
     ]
    }
   ],
   "source": [
    "# Part-A FPGrowth with min threshold = 1e-4\n",
    "model = FPGrowth.train(transactions, minSupport=1e-4, numPartitions=10)\n",
    "result = model.freqItemsets().collect()\n",
    "for fi in result[:10]:\n",
    "    print(fi)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FreqItemset(items=['Slim Essid'], freq=26)\n",
      "FreqItemset(items=['Pinyi Ren'], freq=32)\n",
      "FreqItemset(items=['Michael Fink'], freq=85)\n",
      "FreqItemset(items=['Michael Fink', 'Thomas Eiter'], freq=67)\n",
      "FreqItemset(items=['Michael Fink', 'Hans Tompits'], freq=24)\n",
      "FreqItemset(items=['Keita Matsuo'], freq=22)\n",
      "FreqItemset(items=['Keita Matsuo', 'Leonard Barolli'], freq=22)\n",
      "FreqItemset(items=['Christopher Rasmussen'], freq=26)\n",
      "FreqItemset(items=['Pierre Duhamel'], freq=120)\n",
      "FreqItemset(items=['George Candea'], freq=46)\n"
     ]
    }
   ],
   "source": [
    "# Part-A FPGrowth with min threshold = 1e-5\n",
    "model_b = FPGrowth.train(transactions, minSupport=1e-5, numPartitions=10)\n",
    "result_b = model_b.freqItemsets().collect()\n",
    "for fi in result_b[:10]:\n",
    "    print(fi)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FreqItemset(items=['Wei Wang'], freq=1293)\n",
      "FreqItemset(items=['Wei Zhang'], freq=856)\n",
      "FreqItemset(items=['Lei Zhang'], freq=841)\n",
      "FreqItemset(items=['Wei Li'], freq=805)\n",
      "FreqItemset(items=['H. Vincent Poor'], freq=735)\n",
      "FreqItemset(items=['Jun Wang'], freq=717)\n",
      "FreqItemset(items=['Philip S. Yu'], freq=711)\n",
      "FreqItemset(items=['Wen Gao'], freq=707)\n",
      "FreqItemset(items=['Thomas S. Huang'], freq=691)\n",
      "FreqItemset(items=['Lei Wang'], freq=690)\n"
     ]
    }
   ],
   "source": [
    "# Part-B FPGrowth with min threshold = 0.5e-5\n",
    "model_top5 = FPGrowth.train(transactions, minSupport=0.5e-5, numPartitions=10)\n",
    "result_top5 = model_top5.freqItemsets().collect()\n",
    "result_top5.sort(key=itemgetter(1),reverse=True)\n",
    "for fi in result_top5[:10]:\n",
    "    print(fi)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Part-A FPGrowth with min threshold = 1e-6 {This doesn not work} - Do not run this!\n",
    "# model_d = FPGrowth.train(transactions, minSupport=1e-6, numPartitions=10)\n",
    "# result_d = model_d.freqItemsets().collect()\n",
    "# for fi in result_d[:10]:\n",
    "#    print(fi)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Happens When we successively decrease the Threshold:\n",
    "\n",
    "When we successively decrease the Threshold, the number of association rules increases as number of transactions with min threshold increase and eventually programs runs out of memory and an exception is thrown because of memory spills. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Co-Authors for\n",
      "Rakesh Agrawal\n",
      "['Ramakrishnan Srikant', 'Jerry Kiernan', 'H. V. Jagadish', 'Michael J. Carey', 'Yirong Xu']\n",
      "\n",
      "Jiawei Han\n",
      "['Xifeng Yan', 'Philip S. Yu', 'Jian Pei', 'Yizhou Sun', 'Xin Jin']\n",
      "\n",
      "Zoubin Ghahramani\n",
      "['David L. Wild', 'Katherine A. Heller', 'Michael I. Jordan']\n",
      "\n",
      "Christos Faloutsos\n",
      "['Hanghang Tong', 'Spiros Papadimitriou', 'Jimeng Sun', 'Agma J. M. Traina', 'Caetano Traina Jr.']\n"
     ]
    }
   ],
   "source": [
    "# Question 2-B\n",
    "\n",
    "author = ['Rakesh Agrawal', 'Jiawei Han', 'Zoubin Ghahramani', 'Christos Faloutsos']\n",
    "\n",
    "ra_set = list();\n",
    "jh_set = list();\n",
    "zg_set = list();\n",
    "cf_set = list();\n",
    "\n",
    "# result_top5 is sorted in the order of frequency so code exploits that \n",
    "# feature to calculate top 5 co-authors for given authors\n",
    "for fi in result_top5:\n",
    "    if author[0] in fi[0] and len(ra_set) < 5 and len(fi[0]) > 1:\n",
    "        for k in fi[0]:\n",
    "            if k != author[0] and k not in ra_set:\n",
    "                ra_set.append(k)\n",
    "    if author[1] in fi[0] and len(jh_set) < 5 and len(fi[0]) > 1:\n",
    "        for k in fi[0]:\n",
    "            if k != author[1] and k not in jh_set:\n",
    "                jh_set.append(k)\n",
    "    if author[2] in fi[0] and len(zg_set) < 5 and len(fi[0]) > 1:\n",
    "         for k in fi[0]:\n",
    "            if k != author[2] and k not in zg_set:\n",
    "                zg_set.append(k)\n",
    "    if author[3] in fi[0] and len(cf_set) < 5 and len(fi[0]) > 1:\n",
    "         for k in fi[0]:\n",
    "            if k != author[3] and k not in cf_set:\n",
    "                cf_set.append(k)\n",
    "        \n",
    "print(\"Top 5 Co-Authors for\")\n",
    "print(author[0])\n",
    "print(list(ra_set))\n",
    "print('')\n",
    "print(author[1])\n",
    "print(list(jh_set))\n",
    "print('')\n",
    "print(author[2])\n",
    "print(list(zg_set))\n",
    "print('')\n",
    "print(author[3])\n",
    "print(list(cf_set))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Questions 3\n",
    "# For this question authors are considerd as baskets and venues are transactions where\n",
    "# they have atleast one publication. So Parsing the text to make lookup for author and\n",
    "# venues and then parallelize the list of venues as transactions \n",
    "file = open('publications.txt')\n",
    "\n",
    "author_venue = {}\n",
    "author_keep = []\n",
    "for line in file:\n",
    "        if line.startswith('#@'):\n",
    "            authors = line[2:].split(',')\n",
    "            for i, s in enumerate(authors):\n",
    "                val = s.strip()\n",
    "                if val == '':\n",
    "                    continue\n",
    "                author_keep.append(val)\n",
    "                if val not in author_venue:\n",
    "                    author_venue[val] = set()\n",
    "    \n",
    "            \n",
    "        elif line.startswith('#c'):\n",
    "            publication_venue = line[2:]\n",
    "            venue_keep = publication_venue.strip()\n",
    "            for aut in author_keep:\n",
    "                author_venue[aut].add(venue_keep)\n",
    "            author_keep[:] = []    \n",
    "\n",
    "\n",
    "# Write to file            \n",
    "file = open('dum.txt','w') \n",
    "for key,value in author_venue.items():\n",
    "    str = \"\"\n",
    "    for val in value:\n",
    "        str += val + '~'\n",
    "    \n",
    "    str = str[:-1]\n",
    "    file.write(str)\n",
    "    file.write('\\n')\n",
    "file.close()     \n",
    "\n",
    "# Make an RDD of venues \n",
    "venue_basket = sc.textFile('dum.txt').map(lambda l:list(set(l.split('~'))))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FreqItemset(items=['CoRR'], freq=51437)\n",
      "FreqItemset(items=['IEICE Transactions'], freq=22145)\n",
      "FreqItemset(items=['Bioinformatics'], freq=18669)\n",
      "FreqItemset(items=['ICRA'], freq=17989)\n",
      "FreqItemset(items=['Nucleic Acids Research'], freq=17378)\n",
      "FreqItemset(items=['ICC'], freq=14904)\n",
      "FreqItemset(items=['NeuroImage'], freq=14700)\n",
      "FreqItemset(items=['BMC Bioinformatics'], freq=14448)\n",
      "FreqItemset(items=['GLOBECOM'], freq=14374)\n",
      "FreqItemset(items=['ISCAS'], freq=13498)\n"
     ]
    }
   ],
   "source": [
    "# Questions 3(a) with Threshold = 1e-3\n",
    "model_venue_auth = FPGrowth.train(venue_basket, minSupport=1e-3, numPartitions=1)\n",
    "result_venue_auth = model_venue_auth.freqItemsets().collect()\n",
    "result_venue_auth.sort(key=itemgetter(1),reverse=True)\n",
    "for fi in result_venue_auth[:10]:\n",
    "   print(fi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Questions 3(a) with Threshold = 1e-4 {This fails because of the reason mentioned below}\n",
    "# model_venue_auth = FPGrowth.train(venue_basket, minSupport=1e-4, numPartitions=1)\n",
    "# result_venue_auth = model_venue_auth.freqItemsets().collect()\n",
    "# result_venue_auth.sort(key=itemgetter(1),reverse=True)\n",
    "# for fi in result_venue_auth[:10]:\n",
    "#   print(fi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FreqItemset(items=['CoRR'], freq=51437)\n",
      "FreqItemset(items=['IEICE Transactions'], freq=22145)\n",
      "FreqItemset(items=['Bioinformatics'], freq=18669)\n",
      "FreqItemset(items=['ICRA'], freq=17989)\n",
      "FreqItemset(items=['Nucleic Acids Research'], freq=17378)\n",
      "FreqItemset(items=['ICC'], freq=14904)\n",
      "FreqItemset(items=['NeuroImage'], freq=14700)\n",
      "FreqItemset(items=['BMC Bioinformatics'], freq=14448)\n",
      "FreqItemset(items=['GLOBECOM'], freq=14374)\n",
      "FreqItemset(items=['ISCAS'], freq=13498)\n"
     ]
    }
   ],
   "source": [
    "# Questions 3(a) with Threshold = 0.4e-3\n",
    "model_venue_auth_top10 = FPGrowth.train(venue_basket, minSupport=0.4e-3, numPartitions=1)\n",
    "result_venue_auth_top10 = model_venue_auth_top10.freqItemsets().collect()\n",
    "result_venue_auth_top10.sort(key=itemgetter(1),reverse=True)\n",
    "for fi in result_venue_auth_top10[:10]:\n",
    "   print(fi)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Happens When we successively decrease the Threshold:\n",
    "\n",
    "When we successively decrease the Threshold, the number of association rules increases as number of transactions with min threshold increase and eventually programs runs out of memory and an exception is thrown. Also note that fact that we need to set number of partitions as 1, program fails for partitions greater than one because it results in more memory spill in Spark RDDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 List for Machine Learning\n",
      "['CoRR', 'ICML', 'Neural Computation', 'Journal of Machine Learning Research - Proceedings Track', 'Journal of Machine Learning Research', 'IEEE Trans. Pattern Anal. Mach. Intell.', 'CVPR', 'Neurocomputing', 'Neural Networks', 'IJCAI']\n",
      "\n",
      "Top 10 List for Data Mining \n",
      "['CoRR', 'ICDM', 'CIKM', 'IEEE Trans. Knowl. Data Eng.', 'SDM', 'ICML', 'WWW']\n",
      "\n",
      "Top 10 List for Databases\n",
      "['ICDE', 'SIGMOD Conference', 'CoRR', 'IEEE Trans. Knowl. Data Eng.', 'SIGMOD Record', 'EDBT', 'CIKM', 'IEEE Data Eng. Bull.', 'VLDB J.', 'ACM Trans. Database Syst.']\n",
      "\n",
      "Top 10 List for Computer Networks\n",
      "['GLOBECOM', 'ICC', 'CoRR', 'IEEE/ACM Trans. Netw.', 'IEEE Journal on Selected Areas in Communications', 'Computer Networks', 'Computer Communications', 'ICDCS', 'IEEE Trans. Parallel Distrib. Syst.', 'WCNC']\n",
      "\n",
      "Top 10 List for NLP\n",
      "['COLING', 'LREC', 'CoRR', 'EMNLP', 'HLT-NAACL', 'INTERSPEECH', 'Computational Linguistics']\n"
     ]
    }
   ],
   "source": [
    "vens = ['NIPS', 'KDD', 'VLDB', 'INFOCOM','ACL']\n",
    "\n",
    "machine_learning_list = list();\n",
    "data_mining_list = list();\n",
    "databases_list = list();\n",
    "computer_networks_list = list();\n",
    "nlp_list = list();\n",
    "\n",
    "# result_venue_auth_top10 is sorted in the order of frequency so \n",
    "# this code snippet exploits that fact and finds top 10 venues that\n",
    "# author publish in for each area\n",
    "for fi in result_venue_auth_top10:\n",
    "    if vens[0] in fi[0] and len(machine_learning_list) < 10 and len(fi[0]) > 1:\n",
    "        for k in fi[0]:\n",
    "            if k != vens[0] and k not in machine_learning_list:\n",
    "                machine_learning_list.append(k)\n",
    "    if vens[1] in fi[0] and len(data_mining_list) < 10 and len(fi[0]) > 1:\n",
    "        for k in fi[0]:\n",
    "            if k != vens[1] and k not in data_mining_list:\n",
    "                data_mining_list.append(k)\n",
    "    if vens[2] in fi[0] and len(databases_list) < 10 and len(fi[0]) > 1:\n",
    "        for k in fi[0]:\n",
    "            if k != vens[2] and k not in databases_list:\n",
    "                databases_list.append(k)\n",
    "    if vens[3] in fi[0] and len(computer_networks_list) < 10 and len(fi[0]) > 1:\n",
    "        for k in fi[0]:\n",
    "            if k != vens[3] and k not in computer_networks_list:\n",
    "                computer_networks_list.append(k)\n",
    "    if vens[4] in fi[0] and len(nlp_list) < 10 and len(fi[0]) > 1:\n",
    "        for k in fi[0]:\n",
    "            if k != vens[4] and k not in nlp_list:\n",
    "                nlp_list.append(k)\n",
    "\n",
    "print('Top 10 List for Machine Learning')                \n",
    "print(machine_learning_list)\n",
    "print('')\n",
    "print('Top 10 List for Data Mining ')\n",
    "print(data_mining_list)\n",
    "print('')\n",
    "print('Top 10 List for Databases')\n",
    "print(databases_list)\n",
    "print('')\n",
    "print('Top 10 List for Computer Networks')\n",
    "print(computer_networks_list)\n",
    "print('')\n",
    "print('Top 10 List for NLP')\n",
    "print(nlp_list)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
